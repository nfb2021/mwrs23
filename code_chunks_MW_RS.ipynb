{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74ef1bff-69a2-4c62-8178-7162986d259b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Code chunks for Microwave Remote Sensing\n",
    "\n",
    "Welcome to this Jupyter notebook collecting many code chunks that are useful for the MRS UE exercises. It presents typical Python tools for working with geo data, as also used in professional remote sensing. A focus is put on the first exercise, i.e. classifying forests with optical imagery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a132a1-ab5a-4312-b63c-cc267c5d926a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Some Python libraries you might need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f39d4b7-714a-416a-83e6-a8834d2581cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "A lot of libraries will be used for the exercise, but don't get intimidated by the amout of imports. They mostly fulfill small tasks. \n",
    "\n",
    "First, we import some popular, useful libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f03fe8c-dd71-44cd-9045-a156111fc2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthpy.spatial as es\n",
    "from scipy import signal as sig\n",
    "\n",
    "import cv2\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xar\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fe85bd-7f85-4f1a-b0a2-904130d4f65d",
   "metadata": {},
   "source": [
    "Then, we continue with in-house production software for all sorts of data processing. For more information have a look at https://github.com/TUW-GEO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e555771e-5f9c-4c63-b5a8-0f9087f06c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geospade.crs import SpatialRef\n",
    "from veranda.io.geotiff import write_tiff\n",
    "from veranda.io.geotiff import read_tiff\n",
    "from geopathfinder.folder_naming import build_smarttree\n",
    "from geopathfinder.naming_conventions.sgrt_naming import SgrtFilename\n",
    "from yeoda.products.preprocessed import SIG0DataCube\n",
    "from yeoda.products.parameter import ParameterDataCube\n",
    "#from yeoda.products.base import ProductDataCube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c091425-48d9-4d1d-88cd-0e16552012b6",
   "metadata": {},
   "source": [
    "Several tasks cannot be solely managed with the existing general-purpose packages. Therefore, we have created a helper package dedicated for this lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b7362fe-cec6-4e56-b07c-aa250a13a149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from afe.add_ons import generate_tree\n",
    "from afe.add_ons import OpticalDataCube\n",
    "from afe.geometry import PolygonCollection\n",
    "#from afe.dc_add_ons import TMENPLIADataCube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6158d0-2890-4de0-9cdc-974b70b0c590",
   "metadata": {},
   "source": [
    "Finally, we import `matplotlib` for interactive plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a36185f1-b4cb-4f6b-8fbf-404e7e8fe0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178bd8c5-27d8-4fbb-b30e-9a0bc87641b8",
   "metadata": {},
   "source": [
    "and activate its `widget` backend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "795048bf-ff58-415a-9784-e47d29615053",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget  \n",
    "%matplotlib widget "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22de4ce2-da5d-483d-9dfc-7afd01a4a362",
   "metadata": {},
   "source": [
    "It is generally a good idea to run these widget-commands twice to avoid issues when changing the backend (https://matplotlib.org/2.0.2/faq/usage_faq.html#what-is-a-backend)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b523a5-c185-4c27-bcc9-623350bdb635",
   "metadata": {},
   "source": [
    "To allow for running the code chunks for any user, we can globally define our username here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89baf1e4-a8d2-4e11-b5c1-933595fe5d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e12334178'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USER = os.getcwd().split('/')[2] #This command should automatically get your username\n",
    "USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed76c0ec-9b52-4add-90d4-115db8552285",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating the datacube (Sentinel 1)\n",
    "\n",
    "As we have a bulk of data which occupy quite a few terrabytes of storage space, it would be much too inefficient to simply itterate through the images. To solve this problem we have a yeoda datacube which saves all of the image data in a dataframe like structure, which can be used for managing, filtering and loading bulks of imagery. To learn more about yeoda check out the **yeoda_workshop**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0656bb88-243f-4197-b938-d14a5cf3e104",
   "metadata": {},
   "source": [
    "This section deals with the creation, filtering and handling of sigma naught backscatter (SIG0) and paramter data. `yeoda` offers already datacubes for this kind of tasks, making your life easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762a2d8f-d7b8-43ea-b74d-2b6d64decdf2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SIG0 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39a29b4-dee1-4238-8a31-df9d4f1d3d0b",
   "metadata": {},
   "source": [
    "One important set of the Sentinel-1 data is the sigma naught backscatter (SIG0) background scatter:\n",
    "\n",
    "First, we need to define the path to the files, in this example we will use the 10m resolution but you can also use the 500m data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb7ebf8-4103-4709-8c65-bfd0750ed59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_path = f'/home/{USER}/shared/datasets/fe/data/'\n",
    "rootpath_sig0 = gen_path + f'sentinel1/preprocessed/EU010M'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c3c637-eb95-4e8d-adb0-bb1f9ecd302b",
   "metadata": {},
   "source": [
    "Now we need to define our dimensions of interest, these are \"stored\" in the filename and give information about the data. The ones you see below should be sufficient for your needs, but you can add more according to the field names seen in https://github.com/TUW-GEO/geopathfinder/blob/v0.1.4/src/geopathfinder/naming_conventions/sgrt_naming.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202ccf18-7d78-4d88-aa91-6572db3b0c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions_sig0 = [\"pol\", \"time\", \"orbit_direction\", \"relative_orbit\", \"tile_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f98bb9-c572-4278-a41d-9a7ea494821d",
   "metadata": {},
   "source": [
    "Next we can create a hierarchical folder tree object.\n",
    "\n",
    "The `geopathfinder` library can help you to collect all files within the root folder. To do so, you only need to specify the level names below the root folder and a *regex* pattern for selecting the right files. `geopathfinder`'s `build_smarttree()` function then creates a folder tree object for collecting the files in a recursive manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da1217-cdde-4037-9629-6dd3f1ee3a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_hierarchy = [\"tile\", \"quantity\"]\n",
    "dir_tree_sig0 = build_smarttree(rootpath_sig0, folder_hierarchy, register_file_pattern=\"^[^Q].*.tif\")\n",
    "filepaths_sig0 = dir_tree_sig0.file_register"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa03eff3-c0d1-48b5-bd1c-a92716d94a63",
   "metadata": {},
   "source": [
    "Each file name in this dataset follows a certain naming convention, defined and managed by `geopathfinder`'s class `SgrtFilename`. \n",
    "The list of file paths can then directly be forwarded to `yeoda`'s `SIG0DataCube`, along with the chosen dimensions, the file naming class, and the data encoding attributes (scale factor, no data value, and data type).\n",
    "\n",
    "**Note:** When loading the 500m data you need to set the parameter `sres` to `500`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7620c0-052c-46e5-9182-01ee4018b4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_sig0 = SIG0DataCube(filepaths=filepaths_sig0, dimensions=dimensions_sig0, filename_class=SgrtFilename, scale_factor=100, nodata=-9999, dtype='int16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e5530-cfca-49f0-bcfe-020a54e34d18",
   "metadata": {},
   "source": [
    "The `inventory` property can be used to view the datacube's content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9716de-b080-48e6-bc64-8e47a773c6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_sig0.inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e823da-6677-41a6-8e57-15b60d461957",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Parameter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf3ce24-f4a7-479c-b1e0-f388b7a4fc04",
   "metadata": {},
   "source": [
    "To create a datacube for the parameter data we have to use a different datacube class. The parameter data can be manage in the same way as SIG0 backscatter data, but it is averaged data per Orbit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1517a1-564f-4622-92ed-82b17c146da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath_params = gen_path + f'sentinel1/parameters/EU010M'\n",
    "dir_tree_params = build_smarttree(rootpath_params, folder_hierarchy, register_file_pattern=\"^[^Q].*.tif\")\n",
    "filepaths_params = dir_tree_params.file_register"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b254749-da23-4e4d-8403-23dfb6a89230",
   "metadata": {},
   "source": [
    "Again, we can define the dimensions as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc05d84-382e-486b-96f7-5cb4f89cf2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions_params = [\"relative_orbit\", \"tile\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a7b12e-feba-44ab-8927-4bd9feea9f94",
   "metadata": {},
   "source": [
    "The initialisation works with the `ParameterDataCube` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1290294-115c-4fd2-ad5b-c28872481320",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_params = ParameterDataCube(filepaths=filepaths_params, dimensions=dimensions_params, filename_class=SgrtFilename, sres=500)\n",
    "dc_params.inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c108801c-733a-4db0-88ae-70cb193066e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating the datacube (Sentinel 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375c9571-24e1-4347-9321-173e68337d3e",
   "metadata": {},
   "source": [
    "As with the Sentinel-1 data we can also create a datacube for Sentinel-2 data, again with a different datacube class, in contrast to the Sentinel-1 data we do not need to define a folder hierarchy and register file pattern. Also the dimensions do not have to be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81c295c-f3a1-426b-af54-c4317eef9cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dirpath_s2 = gen_path + f'sentinel2/L2A/EU010M'\n",
    "dir_tree_s2 = generate_tree(root_dirpath_s2)\n",
    "filepaths_s2 = dir_tree_s2.file_register"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9414e5dc-8bd4-465f-a920-373e0cec76d8",
   "metadata": {},
   "source": [
    "The initialisation works with the `OpticalDataCube` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a8a519-0a74-45f5-8331-6cd759cf79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_s2 = OpticalDataCube(filepaths=filepaths_s2)\n",
    "dc_s2.inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2497d94-e776-4b8c-b706-3a0933f75e0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Filtering the Datacube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a64458-02df-415a-931b-d8645bdd82db",
   "metadata": {},
   "source": [
    "The datacube has all the available data saved, as we don't need every single file we can filter the datacube to only contain the files we are interested in.\n",
    "There are five ways to filter the datacube: \n",
    "- `filter_by_dimension`\n",
    "- `filter_by_metadata`\n",
    "- `filter_files_with_pattern`\n",
    "- `filter_spatially_by_geom`\n",
    "- `filter_spatially_by_tilename`\n",
    "\n",
    "Every one of these commands has the keyword `inplace` if this is set to `True` the datacube will be overwritten with the filtered data, if it is set to `False` a new datacube is created containing the filtered data. By deafault it is set to `False`. \n",
    "\n",
    "For the Sentinel-1 and -2 data there are a few tiles available each stretching across an 100x100km area. \n",
    "\n",
    "<img src=\"Tiles.png\" width=\"594\" height=\"420\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b90eb67-6b6d-457a-a724-0d1204aa1671",
   "metadata": {},
   "source": [
    "For this example we will use the datacube for the Sentinel-2 data. We can filter the datacube according to the tile \"E052N015T1\" with the `filter_spatially_by_tilename` function. Beware that the `inplace` key-word specifies, if the datacube should be overwritten (`=True`) or not (`=False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d6210e-d212-44e2-a912-07f372c08ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile = \"E052N015T1\"\n",
    "dc_s2.filter_spatially_by_tilename(tile, inplace=True)\n",
    "dc_s2.inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b1d796-a857-4c33-9e0f-3872a2bcfa8f",
   "metadata": {},
   "source": [
    "Note that we now only have 5433 entries instead of 34055."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4a3291-2a48-4be3-900e-8afa8f4649ff",
   "metadata": {},
   "source": [
    "With the `filter_by_dimension` function we can filter for any available dimension, for example we can filter a specific time period or spectral bands. For Sentinel-1 data we could  also filter after the polarisation, relative orbit or, in case of parameter data, the type of parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad58b034-294d-4091-8ee6-6843ad292d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_bands = dc_s2.SPECTRAL_BANDS\n",
    "dc_s2.filter_by_dimension([(datetime.date(2017, 3, 1), datetime.date(2017, 6, 1))], expressions=[('>','<=')], name='time', inplace=True)\n",
    "dc_s2.filter_by_dimension([\"B02\", \"B03\", \"B04\", \"B08\"], name=\"var_name\", inplace=True)\n",
    "dc_s2.inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb24854-ba17-4d00-9680-979836fb8189",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating the datacube (NDVI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5be320-7f09-4c71-b751-eaadb2637a0b",
   "metadata": {},
   "source": [
    "We can also create a datacube for the Copernicus NDVI data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cc7134-be97-4ffc-89db-f15e50a7755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dirpath_ndvi = gen_path + f'auxiliary_data/copernicus_ndvi/EU500M'\n",
    "dir_tree_ndvi = build_smarttree(root_dirpath_ndvi, folder_hierarchy, register_file_pattern=\"^[^Q].*.tif\")\n",
    "filepaths_ndvi = dir_tree_ndvi.file_register\n",
    "dimensions_ndvi = [\"time\", \"tile\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92573a78-d288-4ccd-86d0-32a6c7e0c257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dc_ndvi = ProductDataCube(filepaths=filepaths_ndvi, dimensions=dimensions_ndvi, filename_class=SgrtFilename)\n",
    "#dc_ndvi.inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c3f5d2-4077-413e-b4de-4f9c047b5b0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading the files into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8545c4da-e8dc-404f-bd05-19acfa412856",
   "metadata": {},
   "source": [
    "To get the data inside of the files we need to load them into memory. Again there is more than one way to do this:\n",
    "- `load_available_bands_by_geom`\n",
    "- `load_available_bands_by_pixels`\n",
    "- `load_by_coords`\n",
    "\n",
    "If you want to load a simple rectangular view of your data `load_available_bands_by_pixels(col, row, col_size, row_size)` does the job perfectly fine. Loading more complex geometries (polygons) can be done with ogr type polygons.\n",
    "\n",
    "To cut the 100x100km tile to an 8x8km area. We can do this by defining the upper left corner and the height and width of the area. (Note: this is in pixels relative to the top left corner of the tile). Beware that row refers to the x-axis and col to the y-axis. Have a look at the Quicklooks to see where your area is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe06f30-1158-4990-9ab1-7215ff642f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ul_col, ul_row = 4800, 900 # The upper left corner of the area relative to the tile\n",
    "col_size, row_size = 800, 800 # The extent of the area [in pixels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fceaac2-b93b-4887-8d3c-8d8561091124",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dc_s2.load_available_bands_by_pixels(ul_col, ul_row, col_size, row_size)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d4b889-d45e-4313-81a6-567c1c3ebee8",
   "metadata": {},
   "source": [
    "**Note:** In case of **Sentinel-1** Datacubes the functions for loading are called:\n",
    "- `load_by_geom`\n",
    "- `load_by_pixels`\n",
    "- `load_by_coords`\n",
    "\n",
    "These functions work the same but you can specify the parameter `dtype` with which you can define in what datastructure (`xarray`=xarray or `numpy`=np.array) you want to load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588c73a3-5e8e-4605-83fa-f14df2b6ff28",
   "metadata": {},
   "source": [
    "We can filter out any time periods where exclusively nan-Values appear in all of the variables by calling the `.dropna` function and sort our data chronologically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27588647-4154-48d0-8e55-89e6aba8edbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(dim=\"time\", how=\"all\")\n",
    "data = data.sortby(\"time\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c49208c-5ed6-4687-b52c-4841ecfb5a01",
   "metadata": {},
   "source": [
    "What you get here is a data structure called `xarray` (https://xarray.pydata.org), featuring spatial (`x`, `y`) and temporal (`time`) coordinates. For each of these coordinates a set of data variables is defined. You can use most functions which you can also use with `numpy` (for example `.mean` or `.dropna` lika above) but you should define a dimension over which it should be calculated. Also you can extract the data of a single variable as a `np.array` with the `.values` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39870a21-8bec-4035-a4b3-bd6684bf8480",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.mean(dim=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4197a420-a9b5-49c6-91f2-46bbf4ce3913",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"B02\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae46bb8e-aec1-402a-8d7f-30208de46f1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading and writing single tiff images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47c0de6-4e4c-42aa-a42a-803018fc4d32",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e36f6d-f3e7-4348-a288-382a8a4222b4",
   "metadata": {},
   "source": [
    "If you want to read a tiff file directly you can use the `read_tiff` function. The return of the function is a tuple with the data as a `np.array` and a `dict` with the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37420ef-a593-4719-9eca-ee9a6aa8aee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = gen_path + f'sentinel1/preprocessed/EU010M/E048N015T1/sig0/D20170111_051741--_SIG0-----_S1BIWGRDH1VVD_095_A0105_EU010M_E048N015T1.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba73fa-a5ed-496a-8c58-cd61ea53a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_arr, tags_dict = read_tiff(src)\n",
    "src_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d156d7-cf3f-4d07-9824-2a1d92540c01",
   "metadata": {},
   "source": [
    "**Note:** When reading data this way it doesnt account for a possible scaling factor or offset, also the no data values are mostly masked. To find out these factors you can look inside the tags dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ef476d-68cd-4faa-aa79-c2520067a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c968d27a-dbf5-47bb-aeb0-3053a79b4edf",
   "metadata": {},
   "source": [
    "You can see that the scaling factor is 100 and the no data values -9999 so we can account for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b205405-b272-4a87-9da1-43168857781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = np.where(src_arr==-9999, np.nan, src_arr/100)\n",
    "data_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9acd629-e84b-432f-bbaa-6a2d1665c6cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Writing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73b962a-2b92-43e8-8988-74feced7a0cd",
   "metadata": {},
   "source": [
    "If you want to write a single tiff file you can use the `write_tiff` function by defining a destination path, the data array and a tags dict. To optimise for storage space you can encode your array, by scaling it and masking the nan values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34543065-d78b-41f1-aee4-58cea4b2fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_arr = np.where(np.isnan(data_in), -9999, data_in*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41fe9ce-8dff-405e-b226-0098fb4e0479",
   "metadata": {},
   "source": [
    "Don't forget to change the datatype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9270a0d-94ad-4209-9f41-ff4041b43045",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_arr = dst_arr.astype(\"int16\")\n",
    "dst_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ce57ac-d43b-469f-93c1-5256e291e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_path = os.getcwd() + '/new_tiff_file.tif'\n",
    "print(dst_path)\n",
    "tags_dst = tags_dict\n",
    "write_tiff(dst_path, src_arr = dst_arr, tags_dict = tags_dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd09e7c6-e207-4de7-bca5-e7b81075f14a",
   "metadata": {},
   "source": [
    "**Note:** If you want to save a rgb image you can use the parameters `red`, `green` and `blue` for the respective values in the `write_tiff` function instead of the `src_arr`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333e3942-1d31-49fe-918e-04327886839e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d368bc3-3081-40ff-ab3b-5f5db67711e5",
   "metadata": {},
   "source": [
    "In case of Sentinel-2 images we can process the image to create rgb images, false color images and increase contrast:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28e6dfb-46f9-4b09-ad6c-9a0d8b65d513",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7596d70-7540-4818-8d7f-f9a13ccde1aa",
   "metadata": {},
   "source": [
    "To properly display an image we need to firstly pick one specific dataset out of our loaded xarray. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bde90f-a411-43ee-8dcb-28e999b3fe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_image = data.sel(time=datetime.date(2017, 4, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99df7481-8191-4919-9a79-55f98e869c53",
   "metadata": {},
   "source": [
    "With the xarray created by loading the Sentinel 2 data we cannot just yet create a meaningful image. As we have already seen in the loaded xarray we have nine different bands available for each time, x and y-coordinate.\n",
    "- B02: Blue\n",
    "- B03: Green\n",
    "- B04: Red\n",
    "- B05: Vegetation red edge\n",
    "- B06: Vegetation red edge \n",
    "- B07: Vegetation red edge \n",
    "- B08: NIR\n",
    "- B11: SWIR\n",
    "- B12: SWIR \n",
    "\n",
    "We have to extract the bands we need from our xarray in order to create an rgb-image. To do this we have to stack the concerned bands with the function `np.stack`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf705bf-3f22-428a-b58f-921d77251afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "b02 = single_image['B02'].values #blue values\n",
    "b03 = single_image['B03'].values #green values\n",
    "b04 = single_image['B04'].values #red values\n",
    "\n",
    "rgb_raw = np.stack([b04, b03, b02], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadabcb9-c123-4355-bc73-658b8b5c122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(rgb_raw)\n",
    "ax.set_title(\"rgb raw\")\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee53a78b-9ea0-4e68-872e-2e09e1db3fd2",
   "metadata": {},
   "source": [
    "As you can clearly see the contrast of the image is very low, which makes it appear dark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e87098-b106-457b-b781-cf94a3e51bfa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185f9799-1395-449b-8012-699720e38666",
   "metadata": {},
   "source": [
    "It is very helpful creating a histogram for your images. A histogram shows the distribution of your pixel values in regards to their color intenensity, meaning low intensities bring dark pixels (black=0) and high intensities bright pixels (white=1).\n",
    "\n",
    "To create a histogram you can use the `hist()` function. Be careful as it only accepts one dimensional arrays. The dimension of an array can be reduced using the `flatten()` function. As we want to show the distribution for each band seperately we have to extract the bands from the rgb-image first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d466b753-fc9d-4776-8811-111406c8e3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rgb_histogram(rgb_im, ax, title, bins = 300):\n",
    "    mbins = np.linspace(np.nanmin(rgb_im), np.nanmax(rgb_im), bins)\n",
    "    ax.hist(rgb_im[:, :, 2].flatten(), color='blue', bins=mbins,  alpha=0.5)\n",
    "    ax.hist(rgb_im[:, :, 1].flatten(), color='green', bins=mbins,  alpha=0.5)\n",
    "    ax.hist(rgb_im[:, :, 0].flatten(), color='red', bins=mbins, alpha=0.5)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('I')\n",
    "    ax.set_ylabel('n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d491db44-c56c-4ee9-bbc7-3711471e8627",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "plot_rgb_histogram(rgb_raw, ax[1], 'Histogram of raw RGB channels', bins=300)\n",
    "ax[0].imshow(rgb_raw)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3c769c-ab82-420b-a83f-b5c209580b36",
   "metadata": {},
   "source": [
    "You can see, that the intensity of the rgb values is very low, which makes it appear dark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9609d1-9059-442f-933a-169e0db3ec80",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Contrast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2281fa12-a6fe-4836-9c8c-23003fcd5cc8",
   "metadata": {},
   "source": [
    "To fix this problem we can increase the contrast of the image. To do this we implement the following functions, which will be needed for processing your image. For more details on how these functions work and why we need them to increase contrast and correctly encode the data, take a look at https://www.cg.tuwien.ac.at/courses/EinfVisComp/Skriptum/SS13/EVC-11%20Point%20Operations.pdf for a great overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24652897-e70e-4085-8ac0-ee20b5dc468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_clip(I, percentile=0.02, pooled=True):\n",
    "    \"\"\" \n",
    "    Calculates the quantiles of I using the percentile parameter and clips the values using the clip function defined below.\n",
    "    \n",
    "    Modifies I\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    I : np.array(rows, cols, bands)\n",
    "        Image array.\n",
    "    percentile : float, optional\n",
    "        Percentile defining the clipping boundaries of I in terms of its distribution (defaults to 0.02). \n",
    "    pooled: if True, computes the pooled percentile over all band\n",
    "            (default, use this to keep the relative intensities of the bands for natural looking images)    \n",
    "            if False, computes the percentiles for each band individually\n",
    "            (use this - in conjunction with stretch - to bring the different bands into a comparable range, e.g. for false colour images)\n",
    "        \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.array : \n",
    "        Auto-clipped image data.\n",
    "    \n",
    "    \"\"\"\n",
    "    if pooled:\n",
    "        v_min = np.nanquantile(I, percentile)\n",
    "        v_max = np.nanquantile(I, 1 - percentile)\n",
    " \n",
    "    else:\n",
    "        tmp = I.reshape(-1, I.shape[-1]) #collapes image x,y 2d-array into a 1d-array\n",
    "        v_min = np.nanquantile(tmp, percentile, axis=0)\n",
    "        v_max = np.nanquantile(tmp, 1 - percentile, axis=0)\n",
    "        \n",
    "    return clip(I, v_min, v_max)        \n",
    "\n",
    "def clip(I, v_min, v_max):\n",
    "    \"\"\" \n",
    "    Performs clipping (dt. \"Histogrammbegrenzung\")\n",
    "    Sets all values in I that are outside of [v_min, v_max] to the corresponding boundary.\n",
    "\n",
    "    \n",
    "    Modifies I\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    I : np.array\n",
    "        Image array.\n",
    "    v_min : scalar or array\n",
    "        Lower clipping boundary for each band\n",
    "    v_max : scalar or array\n",
    "        lower clipping boundary for each band\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.array : \n",
    "        Clipped image data.\n",
    "        \n",
    "    \"\"\"\n",
    "        \n",
    "    tmp = I.reshape(-1, I.shape[-1]) #collapes image x,y 2d-array into a 1d-array         \n",
    "    if np.isscalar(v_min):\n",
    "        tmp[tmp < v_min] = v_min\n",
    "        tmp[tmp > v_max] = v_max\n",
    "    else:\n",
    "        idx = np.where(tmp < v_min)\n",
    "        tmp[idx]=v_min[idx[1]]\n",
    "        idx = np.where(tmp > v_max)\n",
    "        tmp[idx]=v_max[idx[1]]        \n",
    "    \n",
    "    return I\n",
    "            \n",
    "def stretch(I, p_min, p_max, pooled=True):\n",
    "    \"\"\"\n",
    "    Performs histogram stretching or normalisation (dt. \"Spreizung\")\n",
    "    Computes and applies an affine transformation of values in I to the range [p_min, p_max]. \n",
    "    For floating point images to be displayed with pylab.imshow(), p_min=0, p_max=1\n",
    "    should be chosen.\n",
    "    \n",
    "    Modifies I\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    I : np.array\n",
    "        Image array.\n",
    "    p_min : number\n",
    "        Lower boundary of the output range.\n",
    "    p_max : number\n",
    "        Upper  boundary of the output range.\n",
    "    pooled: if True, the transformation is computed for and applied to all bands simultaneously  \n",
    "            if False, -\"- to the individual bands separately\n",
    "    Returns\n",
    "    -------\n",
    "    np.array : \n",
    "        Normalised image data within the range [p_min, p_max].\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    tmp = I.reshape(-1, I.shape[-1]) #collapes image x,y 2d-array into a 1d-array   \n",
    "\n",
    "    if pooled:    \n",
    "        q_min = np.nanmin(I)\n",
    "        q_max = np.nanmax(I)\n",
    "\n",
    "    else:\n",
    "             \n",
    "        q_min = np.nanmin(tmp, axis = 0)\n",
    "        q_max = np.nanmax(tmp, axis = 0)        \n",
    "\n",
    "    tmp[:] =  (p_max - p_min) * (tmp - q_min) / (q_max - q_min) + p_min\n",
    "    return I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce379d4a-e0c0-4d03-9942-40e60ab36e95",
   "metadata": {},
   "source": [
    "First we can use the `auto_clip()` or `clip()` function to set values of intensity which are outside of a boundary to the boundary border: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce0be7b-89b2-40a7-bea7-fff3944e5781",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_clipped = auto_clip(rgb_raw.copy(), percentile=0.05, pooled = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4197c9ae-c699-47cd-b0c1-6ec377fbb483",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "plot_rgb_histogram(rgb_clipped, ax[1], 'Histogram of clipped RGB channels', bins=300)\n",
    "ax[0].imshow(rgb_clipped)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bac520-5810-4321-a041-87244e8de7ee",
   "metadata": {},
   "source": [
    "You can see that the histogram now has clear borders, and all the values below/above these borders have been set to the border values. You can play around clipping fewer/more values by setting the percentile value in the `auto_clip()` function. The `pooled` parameter sets if the bands get clipped individually (`=FALSE`) or as a group (`=TRUE`).\n",
    "\n",
    "As the intensity of the values is still too low after clipping we have to stretch the values to spread between 0 and 1. We can do this by using the `stretch()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b470558-dcc1-4c07-a5f6-aa421ae40790",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_clipped_stretched = stretch(rgb_clipped.copy(), 0, 1, pooled = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e270ccfa-2fcf-4ad3-b679-72055e29d79c",
   "metadata": {},
   "source": [
    "Again, we can extract the bands, flatten them and create a histogram. Also we will compare the histogram to the raw image, the only clipped image and an only stretched image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9642a94-974d-474c-96f7-d02dbd1d7f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "plot_rgb_histogram(rgb_clipped_stretched, ax[1], 'Histogram of raw RGB channels', bins=300)\n",
    "ax[0].imshow(rgb_clipped_stretched)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafd59b3-cb62-4dc5-86f4-1ae6771195fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_stretched = stretch(rgb_raw.copy(),0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc742986-3d8a-4e93-a900-a6f0052073e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,2, figsize=(8,12))\n",
    "plot_rgb_histogram(rgb_raw, ax[0,1], 'Histogram of raw RGB channels', bins=300)\n",
    "ax[0,0].imshow(rgb_raw)\n",
    "#ax[0,0].set_axis_off()\n",
    "\n",
    "plot_rgb_histogram(rgb_clipped, ax[1,1], 'Histogram of clipped RGB channels', bins=300)\n",
    "ax[1,0].imshow(rgb_clipped)\n",
    "ax[1,0].set_axis_off()\n",
    "\n",
    "\n",
    "plot_rgb_histogram(rgb_clipped_stretched, ax[2,1], 'Histogram of clipped/stretched RGB channels', bins=300)\n",
    "ax[2,0].imshow(rgb_clipped_stretched)\n",
    "ax[2,0].set_axis_off()\n",
    "\n",
    "\n",
    "plot_rgb_histogram(rgb_stretched, ax[3,1], 'Histogram of stretched RGB channels', bins=300)\n",
    "ax[3,0].imshow(rgb_stretched)\n",
    "ax[3,0].set_axis_off()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6658fc2-ecb2-45f2-a6e9-4ecf38d6c5c0",
   "metadata": {},
   "source": [
    "Now we have a good distribution of the values between 0 and 1 and should receive a clear image when plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd96e6c-0c22-4956-9608-1acbeaf78cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(rgb_clipped_stretched)\n",
    "ax.set_title(\"rgb clipped stretched\")\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63857e20-29b2-4ece-880a-89cb6f09b976",
   "metadata": {},
   "source": [
    "If the final image doesnt look natural or still seems too dark try playing around with the percentile value or set the intensity value manually with the `clip()` function instead of `auto_clip()`. If an individual band has a much higher intensity compared to the others try setting `pooled` to `=FALSE` when clipping and stretching. \n",
    "\n",
    "You can also use `np.log()` on the raw rgb image before clipping and stretching it to increase contrast.\n",
    "\n",
    "You can see what happens when we play around with the `percentile` parameter in the `auto_clip` function or use the `np.log` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d1fdb-14f0-485a-9ab7-398393a8a65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_clipped_stretched_01 = stretch(auto_clip(rgb_raw.copy(), percentile=0.01),0,1)\n",
    "rgb_clipped_stretched_05 = stretch(auto_clip(rgb_raw.copy(), percentile=0.05),0,1)\n",
    "rgb_clipped_stretched_15 = stretch(auto_clip(rgb_raw.copy(), percentile=0.15),0,1)\n",
    "rgb_clipped_stretched_log = stretch(auto_clip(np.log(rgb_raw.copy()), percentile=0.05),0,1)\n",
    "\n",
    "fig,ax = plt.subplots(2,2,figsize=(10,10))\n",
    "ax[0,0].imshow(rgb_clipped_stretched_01)\n",
    "ax[0,0].set_title(\"percentile = 0.01\")\n",
    "ax[0,1].imshow(rgb_clipped_stretched_05)\n",
    "ax[0,1].set_title(\"percentile = 0.05\")\n",
    "ax[1,0].imshow(rgb_clipped_stretched_15)\n",
    "ax[1,0].set_title(\"percentile = 0.15\")\n",
    "ax[1,1].imshow(rgb_clipped_stretched_log)\n",
    "ax[1,1].set_title(\"log and percentile = 0.05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765df428-149e-4102-8fcb-1bd89bf06385",
   "metadata": {
    "tags": []
   },
   "source": [
    "### False color image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3ccf78-10ee-43e0-b3d4-a9c0a709c464",
   "metadata": {},
   "source": [
    "As we want to analyze our picture regarding forested areas, we can make this easier using a false color image. Vegetetation reflects light in much higher intensities in the near infrared side of the spectrum. To visualize this we can replace the red band in the rgb image with the NIR band (=band 8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56e5b2f-631f-40f4-99cb-53516dd358ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "b02 = single_image['B02'].values #blue values\n",
    "b03 = single_image['B03'].values #green values\n",
    "b08 = single_image['B08'].values #NIR values\n",
    "\n",
    "fc_img = np.stack([b08, b03, b02], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107dff4d-dcdf-4d80-bf8f-734e9d55b252",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize = (8,4))\n",
    "ax[0].imshow(fc_img)\n",
    "ax[0].set_title(\"false-color image\")\n",
    "ax[0].set_axis_off()\n",
    "plot_rgb_histogram(fc_img, ax[1], 'Histogram of false-color image', bins=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b28970-7329-4450-9c86-bbb6e808394b",
   "metadata": {},
   "source": [
    "We can see that the NIR values are slightly higher than the ones of the green and blue band. So this time we will change the `pooled` parameter to `False` when incresing the contrast. You can also see the difference when the `pooled` parameter is set to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb4636-4c24-4a4f-afd9-941a88ec4b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_img_clipped_stretched_false = stretch(auto_clip(np.log(fc_img.copy()), pooled = False), 0, 1, pooled = False)\n",
    "\n",
    "fc_img_clipped_stretched_true = stretch(auto_clip(np.log(fc_img.copy()), percentile=0.005, pooled = True), 0, 1, pooled = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d669d3-4735-4549-9f08-c117b0c82477",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize = (10,10))\n",
    "\n",
    "ax[0,0].imshow(fc_img_clipped_stretched_false)\n",
    "ax[0,0].set_title(\"false-color image clipped stretched\\npooled=False\")\n",
    "ax[0,0].set_axis_off()\n",
    "\n",
    "plot_rgb_histogram(fc_img_clipped_stretched_false, ax[0,1], 'Histogram pooled=False', bins=300)\n",
    "\n",
    "ax[1,0].imshow(fc_img_clipped_stretched_true)\n",
    "ax[1,0].set_title(\"false-color image clipped stretched\\npooled=True\")\n",
    "ax[1,0].set_axis_off()\n",
    "\n",
    "plot_rgb_histogram(fc_img_clipped_stretched_true, ax[1,1], 'Histogram pooled=True', bins=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78628879-3f7c-479e-8be8-9e3b06aaabdc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cloud filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57367ff6-08d7-43be-a58c-b0708b7683ac",
   "metadata": {},
   "source": [
    "In this part of the task, we want to filter out any visible clouds. For this we need images from five consecutive dates, so be sure that your xarray has data from at least five different dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a77eb8-5c69-4b21-a8af-a2f3a3ecea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd57f508-8f11-4380-8f1c-5e3595a43e02",
   "metadata": {},
   "source": [
    "We will choose the dates 1.4, 21.4, 11.5, 18.5 and 28.5.2017 for our example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a0eaa-def6-48e2-a3f8-400819424a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = data.time[2:]\n",
    "consecutive_imgs = data.sel(time=time)\n",
    "consecutive_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e011e73-a8d3-4dbf-9dd9-ea890383cf0f",
   "metadata": {},
   "source": [
    "We now stack and increase the contrast of every image, and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45817a27-27cf-4f7a-9c93-412cd8838969",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "i=0\n",
    "for i in range(5):\n",
    "    img = consecutive_imgs.sel(time=consecutive_imgs.time[i])\n",
    "    b02 = img['B02'].values\n",
    "    b03 = img['B03'].values\n",
    "    b04 = img['B04'].values\n",
    "    rgb_raw = np.stack([b04, b03, b02], axis=2)\n",
    "    rgb = stretch(auto_clip(np.log(rgb_raw)),0,1)\n",
    "    imgs.append(rgb)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8662f27e-a151-4542-985f-3c58e2ad2a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3, figsize=(10,8))\n",
    "ax = ax.ravel()\n",
    "i = 0\n",
    "\n",
    "for img in imgs:\n",
    "    ax[i].imshow(img)\n",
    "    ax[i].set_axis_off()\n",
    "    ax[i].set_title(consecutive_imgs.time[i].values)\n",
    "    i +=1\n",
    "\n",
    "ax[5].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8923f4b0-f284-4659-a656-6b8e96a2a208",
   "metadata": {},
   "source": [
    "Now we can calculate the median pixel value from these images to receive a cloudless image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f78b53-cf97-48aa-92f2-1e86a41eb799",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudfree_img_data = consecutive_imgs.median(dim=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0572e701-9155-4879-b0d1-05f3660dc7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "b02 = cloudfree_img_data['B02'].values\n",
    "b03 = cloudfree_img_data['B03'].values\n",
    "b04 = cloudfree_img_data['B04'].values\n",
    "\n",
    "cloudfree_img_raw = np.stack([b04, b03, b02], axis=2)\n",
    "cloudfree_img = stretch(auto_clip(np.log(cloudfree_img_raw)),0,1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "ax.imshow(cloudfree_img)\n",
    "ax.set_title(\"Cloudfree image\")\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4e709e-e14a-498a-9af3-6ef8d3cf7bfd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NDVI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac99cd4-05e6-4bef-a8e5-56f99131881d",
   "metadata": {},
   "source": [
    "NDVI and many other indices rely on the normalized difference, represented by the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc4bb3e-cd14-4ceb-87f5-4852207bfc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_difference(a, b):\n",
    "    return (a - b) / (a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284d93a8-c543-41b6-9c07-5dc5a013444c",
   "metadata": {},
   "source": [
    "To get the ndvi we need to calculate the normalized difference between the infrared (B08) and the red (B04) band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eb8edd-5383-413f-83bd-cf0aa796f8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_img = normalized_difference(cloudfree_img_data[\"B08\"].values, cloudfree_img_data[\"B04\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eecaec-2c32-409b-9301-40b471f89f71",
   "metadata": {},
   "source": [
    "We can plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5924efbb-e576-43d9-99d1-9a852a04b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "ax.set_title(\"NDVI\")\n",
    "im = ax.imshow(ndvi_img, cmap=\"RdYlGn\")\n",
    "ax.set_axis_off()\n",
    "plt.colorbar(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e13c42-e58e-41c5-90c0-3e4188d1be72",
   "metadata": {},
   "source": [
    "NDVI does a good job seperating vegetation from non-vegetation but it can't seperate forest from vegetated cropland, or grassland."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb27b1f6-38f0-4bb3-8945-23f893a40cd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating custom colormaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deb36c5-a927-4075-a46e-87f7e015ebb6",
   "metadata": {},
   "source": [
    "Creating custom colormaps, can be helpful to visualise and analyse classification outcomes. `matplotlib`'s `ListedColormap` allows you to create a colormap from a list containing `n` colors.\n",
    "\n",
    "**Hint:** Using RGBA values for colors, some pixels can be set to be fully transparent. This can be useful if you want to overlay two images. You might also want to take a look at the `set_under` and `set_over` methods of colormaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d120b342-df03-4ba1-a039-29fa09b0b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = colors.ListedColormap(['red', 'blue', 'turquoise', 'white'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19378ef9-dd93-4b9a-b690-d7c87239f987",
   "metadata": {},
   "source": [
    "The value ranges referring to each of the colors must be specified in a second array with a length of `n+1` defining the upper and lower value boundary allocated to a certain color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeacaf4e-375f-41cb-8328-d32b1e4f7fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = np.array([-1000, -12, -8, -6, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aa98c3-3151-4d8a-9e02-db2cbf03d363",
   "metadata": {},
   "source": [
    "We can also add labels for the ticks and change their location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643e0df5-b163-47c8-b5d2-6873aab328da",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['red', 'blue', 'turquoise', 'white']\n",
    "tick_location = [-500, -10, -7, -3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877669c3-7e5e-44bb-92fd-eba70a02c0bc",
   "metadata": {},
   "source": [
    "The number of elements in the colormap `cmap.N` and the class boundaries can then be used to create an object for normalising the values to 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a835b2d9-c225-4171-99e9-a6c62943df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = colors.BoundaryNorm(bounds, cmap.N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9233dd75-7263-43f8-b241-f48355926fa3",
   "metadata": {},
   "source": [
    "Now we can plot an image with the discrete colormap of our choice. For this Example we will use our previously loaded single tiff image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afccfb9-27ed-49a8-90ca-02d3f3f1d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot()\n",
    "img_col = ax.imshow(data_in, cmap=cmap, norm =norm)\n",
    "\n",
    "cbar = plt.colorbar(img_col, ax=ax, boundaries=bounds, ticks=tick_location, shrink=0.65, orientation='horizontal')\n",
    "cbar.set_label('Sigma nought backscatter')\n",
    "cbar.set_ticklabels(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cf3a38-4e37-4186-ab1f-fe20b43db174",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DEM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850246c-8f1b-4dca-9f4f-3a5acac1f330",
   "metadata": {},
   "source": [
    "First, we need to locate where the digital elevation model (DEM) data is stored and select a file of interest (10000x10000 pixels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2cd51e-fe99-4f0c-9904-eda799f07177",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_path = gen_path + f'auxiliary_data/dem/srtm_vfp/EU010M/{tile}/SRTM_VFP_EU010M_{tile}.tif'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfa3468-95a0-499c-a2b5-c0a78edc368f",
   "metadata": {},
   "source": [
    "Then we can load it with `read_tiff` as a `numpy` array. The first return value is the data, the second metadata tags, which we do not need now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d8836-7184-4414-835e-c0c2258c2dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem, _ = read_tiff(dem_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb88a94-cb56-402d-95ea-b93a6a2fb55b",
   "metadata": {},
   "source": [
    "Then we select elevation data for a specific region of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683c5a97-9302-437f-a947-87ae39a91bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation = dem[ul_col:ul_col+col_size, ul_row:ul_row+row_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c715f674-1d73-4805-bae2-aadcfc801b43",
   "metadata": {},
   "source": [
    "Next, we can create a hillshade from the selected elevation data. The `earthpy` library features a function for hillshading that takes elevation data, and an azimuth and altitude value defining the source of illumination. Note that for larger arrays, this operation might take a while. Hillshade is used for visualisation purposes. An azimuth of 0/360 means the illumination is from northern, an azimuth of 180 means it is from southern direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66406f3d-e923-4497-b3ba-29bac37d974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hillshade1 = es.hillshade(elevation, azimuth=45, altitude=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8b617f-a06b-41e3-a9d4-ff0f0c0c6916",
   "metadata": {},
   "source": [
    "In order to compute the aspect (orientation) of the surfaces, we compute the first derivative in north-south direction, which is realised by convoluting DEM data with a vertical Sobel kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd812c9b-c7e1-4e9c-bf44-8e472623a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sob_v = np.array([[1, 2, 1],[0, 0, 0],[-1, -2, -1]], dtype = 'int32')\n",
    "gradx = sig.convolve2d(elevation, sob_v, mode='same')/8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14792b5-bf3c-4ebe-a6a7-754ea911741c",
   "metadata": {},
   "source": [
    "Finally, we can plot the created hillshade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539a33dc-727c-4686-8033-df0fddd11b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = colors.ListedColormap([\"grey\", 'yellow'])\n",
    "\n",
    "fig = plt.figure(figsize=(11,8))\n",
    "ax0 = plt.subplot(231)\n",
    "ax1 = plt.subplot(232)\n",
    "ax2 = plt.subplot(233)\n",
    "\n",
    "ax0.set_title('Elevation in m')\n",
    "el_img = ax0.imshow(elevation, cmap=\"gist_earth\")\n",
    "fig.colorbar(el_img, ax=ax0, shrink=0.75, orientation='horizontal')\n",
    "ax1.set_title('Hillshade 45 AZ')\n",
    "hs_img = ax1.imshow(hillshade1, cmap=\"Greys_r\",vmin=0, vmax=255)\n",
    "ax2.set_title('Nord-Sd Gradient')\n",
    "hs_img = ax2.imshow(gradx, vmin=-10, vmax=10)\n",
    "\n",
    "ax3 = plt.subplot(234)\n",
    "ax4 = plt.subplot(235)\n",
    "ax5 = plt.subplot(236)\n",
    "\n",
    "ax3.set_title('southern facing slope')\n",
    "ax3.imshow(gradx > 2, cmap = cmap)\n",
    "ax4.set_title('northern facing slope')\n",
    "ax4.imshow(gradx < -2, cmap = cmap)\n",
    "ax5.set_title('no north-south facing slope')\n",
    "hs_img = ax5.imshow(np.abs(gradx) < 2, cmap = cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4f0b5d-0eb7-4e09-9402-a487b9db7747",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 500m Daten "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383110ad-10e9-4455-9fde-ff05abc1ea77",
   "metadata": {},
   "source": [
    "The following code is an example to demonstrate the differences in code when using on 500m data.\n",
    "In order to better understand the Tiling Convention it migth be adviseable to have a look at the following PowerPoint.\n",
    "https://cartography.tuwien.ac.at/eurocarto/wp-content/uploads/2015/09/3_6_ppt.pdf Especially the pages 12 to 17 should help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49851113-6f24-4051-82bc-bb9a07951a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath_sig0_500m = gen_path + f'sentinel1/preprocessed/EU500M'\n",
    "dimensions_sig0 = [\"pol\", \"time\", \"orbit_direction\", \"relative_orbit\", \"tile_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5fbf7c-6e13-4c37-a771-e828c695b83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_hierarchy = [\"tile\", \"quantity\"]\n",
    "dir_tree_sig0_500m = build_smarttree(rootpath_sig0_500m, folder_hierarchy, register_file_pattern=\"^[^Q].*.tif\")\n",
    "filepaths_sig0_500m = dir_tree_sig0_500m.file_register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7288b5-b52f-4873-9d42-ba7c96581956",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_sig0_500m = SIG0DataCube(filepaths=filepaths_sig0_500m, dimensions=dimensions_sig0, filename_class=SgrtFilename, scale_factor=100, nodata=-9999, dtype='int16', sres=500)\n",
    "dc_sig0_500m.inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42d3292-4111-42a6-81b5-49748f25cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile = 'E042N006T6'\n",
    "dc_sig0_500m.filter_by_dimension(tile, name=\"tile_name\", inplace=True)\n",
    "dc_sig0_500m.inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab697e1-27f8-4be3-a192-9f758f157670",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_sig0_500m.filter_by_dimension([(datetime.datetime(2017, 3, 1), datetime.datetime(2017, 3,5))], expressions=[('>','<=')], name='time', inplace=True)\n",
    "dc_sig0_500m.inventory.sort_values(by=['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844a0cc4-087a-4516-88d9-86d45da11a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "ul_col, ul_row = 0, 0 # The upper left corner of the area relative to the tile\n",
    "col_size, row_size = 1200, 1200 # The extent of the area [in pixels] maximum 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f399883b-dcf2-4de2-a023-397ee2eb3ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_500m = dc_sig0_500m.load_by_pixels(ul_col, ul_row, col_size, row_size)\n",
    "data_500m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00262bbe-956a-416d-8ed8-d758f6340e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = data_500m.time[6]\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe2a2c-9004-4545-adae-939bc1947875",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_image = data_500m.sel(time=date)\n",
    "single_image['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05479c8c-36f4-43ad-8b4b-328e9395bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(single_image['1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080eaa00-2edc-441f-839f-17ae920385f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afe23w",
   "language": "python",
   "name": "afe23w"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data management with *yeoda* (v0.3.0)\n",
    "[yeoda](https://pypi.org/project/yeoda/) (**y**our **e**arth **o**bservation **d**ata **a**ccess) is a geopspatial data management library developed at the GEO Department at TU Wien for handling earth observation (EO) data. It lets you read data saved on your disk as a file (netCDF, GeoTIFF) and makes it available in datacubes. Datacubes contain standard operations, e.g., filtering, sorting, selecting, and make the data easily processable with standard Python libraries, such as *numpy* or *xarray*. \n",
    "\n",
    "In this notebook the general handling of *yeoda* datacubes will be explained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a datacube\n",
    "\n",
    "A *yeoda* datacube augments existing data by integrating it into a datacube architecture similar to, e.g. [Open Data Cube](https://www.opendatacube.org/about). To set up such a datacube, *yeoda* works in concert with [geopathfinder](https://github.com/TUW-GEO/geopathfinder) (file naming), [veranda](https://github.com/TUW-GEO/veranda) (IO classes) and [pytiletproj/Equi7Grid](https://github.com/TUW-GEO/Equi7Grid) (geo-referencing).\n",
    "\n",
    "First, collect the files you want to put into your datacube. You can use [geopathfinder](https://github.com/TUW-GEO/geopathfinder) to conveniently gather files matching a certain file naming convention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from geopathfinder.folder_naming import build_smarttree\n",
    "\n",
    "USER = os.getcwd().split('/')[2]\n",
    "root_path = f'/home/{USER}/shared/datasets/fe/data/sentinel2/L2A'\n",
    "folder_hierarchy = [\"sub_grid\", \"tile_name\", \"var_name\"]\n",
    "\n",
    "# regex expressions are supported to select only files matching a certain pattern\n",
    "# (i.e. not starting with Q ending with .tif)\n",
    "tree = build_smarttree(root_path, folder_hierarchy, register_file_pattern=\"^[^Q].*.tif$\")\n",
    "filepaths = tree.file_register\n",
    "\n",
    "print(f\"{len(filepaths)} files registered:\")\n",
    "print(\"\\n\".join(filepaths[:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use [pytiletproj/Equi7Grid](https://github.com/TUW-GEO/Equi7Grid) to define a grid to be used by the datacube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from equi7grid.equi7grid import Equi7Grid\n",
    "\n",
    "subgrid = Equi7Grid(10).EU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file registry and grid can now be used directly as input to *yeoda's* `EODataCube` constructor, to wrap a datacube structure around our files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopathfinder.naming_conventions.acube_naming import ACubeFilename\n",
    "from yeoda.datacube import EODataCube\n",
    "\n",
    "dimensions = [\"var_name\", \"dtime_1\", \"dtime_2\", \"tile_name\"]\n",
    "s2_cube = EODataCube(filepaths=filepaths, dimensions=dimensions, filename_class=ACubeFilename, grid=subgrid,\n",
    "                     sdim_name=\"tile_name\", tdim_name=\"dtime_1\")\n",
    "s2_cube.inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're all setup and can perform operations on your freshly minted datacube. Internally, *yeoda* uses a [GeoPandas](https://geopandas.org) dataframe to store the filename and geometry information.  On top of that, datacube functions were defined to filter, split, sort, align, etc. the data. It has to be noted that most functions have a keyword argument `inplace`, same as most [GeoPandas](https://geopandas.org) functions. In the next sections some example usages of these functions will be shown.\n",
    "\n",
    "This example showcases the most generic flavour of a datacube, however there are also more specialized data cube classes available, which are tailored towards the products operated by the research group Remote Sensing of the GEO Department at TU Wien (TUWGEO). See the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up product specific datacubes\n",
    "\n",
    "To work with preprocessed data you can use the classes `SIG0DataCube` for sigma nought and `GMRDataCube` for radiometric terrain-flattened gamma nought data. On the value-added data side, `SSMDataCube` allows you to access the TUWGEO SSM data, and `SCATSARSWIDataCube` SWI data, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopathfinder.naming_conventions.sgrt_naming import SgrtFilename\n",
    "from yeoda.products.base import ProductDataCube\n",
    "\n",
    "root_path = f\"/home/{USER}/shared/datasets/fe/data/sentinel1/preprocessed/EU500M\"\n",
    "folder_hierarchy = [\"tile_name\", \"var_name\"]\n",
    "\n",
    "tree = build_smarttree(root_path, folder_hierarchy, register_file_pattern=\"^[^Q].*.tif$\")\n",
    "dimensions = [\"time\", \"var_name\", \"tile_name\", \"pol\"]\n",
    "scale_factor = 100 # with yeoda v0.3.0, the scale factor still needs to be defined by the user\n",
    "sig0_cube = ProductDataCube(filepaths=tree.file_register, dimensions=dimensions, filename_class=SgrtFilename, \n",
    "                            grid=Equi7Grid(500).EU, scale_factor=scale_factor)\n",
    "sig0_cube.inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that *yeoda* is not limited to GeoTIFF files, it also supports NetCDF files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension operations\n",
    "\n",
    "The following sections shows how you can manipulate the dimensions of the datacube itself, before doing any further operations based on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming dimensions\n",
    "\n",
    "If you have to work with a pre-defined naming convention in *geopathfinder* (e.g. the *yeoda* naming convention) and if you do not agree with the naming of the filename parts/dimensions, you can still rename dimensions afterwards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig0_cube.rename_dimensions({'tile_name': 'tile'}, inplace=True)\n",
    "sig0_cube.inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding dimensions\n",
    "\n",
    "You can simply add new filepath-dependent values (e.g. file size, cloud coverage, …) along a new dimension (e.g. named “new_dimension”) with a few lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended = sig0_cube.add_dimension(\"ones\", [1] * len(sig0_cube))\n",
    "extended.inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting\n",
    "\n",
    "One of the most common operations is to sort the inventory according to some metadata, e.g. the timestamp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_descending = sig0_cube.sort_by_dimension('time', ascending=False)\n",
    "sorted_descending.inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "\n",
    "Once you have your datacube structure setup you can also filter it before doing any processing. For instance, if you want to do some runtime intensive processing on only a small portion of the data. The following sections give a few examples of the available filtering methods. Again most methods provide a `inplace` flag, similar to [GeoPandas](https://geopandas.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by geometry\n",
    "\n",
    "You can filter for arbitrary geometry or a list of bounding box coordinates. The filtered cube will only contain files within the specified geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import osr\n",
    "\n",
    "sref = osr.SpatialReference()\n",
    "sref.ImportFromEPSG(4326)  # LonLat spatial reference system\n",
    "\n",
    "bbox_inside = [(12.628, 46.385), (15.768, 48.431)]  # [(x_min, y_min), (x_max, y_max)]\n",
    "filtered_by_bbox = sig0_cube.filter_spatially_by_geom(bbox_inside, sref=sref)\n",
    "print(f\"Number of filtered files with a bbox located inside the data tiles: {len(filtered_by_bbox)}\")\n",
    "\n",
    "bbox_outside = [(4.404, 44.443), (8.826, 47.811)]\n",
    "filtered_by_bbox = sig0_cube.filter_spatially_by_geom(bbox_outside, sref=sref)\n",
    "print(f\"Number of filtered files with a bbox located outside the data tiles: {len(filtered_by_bbox)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by dimension\n",
    "\n",
    "A very important function is `filter_by_dimension`, which accepts a list of values and expressions to filter the data along a dimension. The list of `expressions` has the same length as the values list and includes mathematical comparison operators, e.g. `“==”`, `“<=”`, `“>=”`, `“<”`, `“>”` (`“==”` is default). Some examples are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only consider VV polarisation\n",
    "only_vv = sig0_cube.filter_by_dimension(['VV'], name=\"pol\")\n",
    "only_vv.inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# only consider data between 2019-02-01 and 2019-03-01\n",
    "time_span = [(datetime(2019, 2, 1), datetime(2019, 3, 1))]\n",
    "time_span_only = sig0_cube.filter_by_dimension(time_span, [('>=', '<')], name='time')\n",
    "time_span_only.inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by file pattern\n",
    "\n",
    "You can also directly filter on the filename using a regex pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_by_pattern = sig0_cube.filter_files_with_pattern(\".*_066_.*\")\n",
    "filtered_by_pattern.inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting\n",
    "\n",
    "You can use split operations to segregate your datacube into chunks, which can then be used for processing. For instance, you could split data into months and calculate monthly means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split by dimension\n",
    "\n",
    "Split a datacube based on dimension values. The splitting conditions are expressed the same way as in `filter_by_dimension`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = ['VV', 'VH']\n",
    "vv_cube, vh_cube = sig0_cube.split_by_dimension(values, name=\"pol\")\n",
    "print(f\"Parent datacube of length {len(sig0_cube)}, split into two datacubes of length {len(vv_cube)} and {len(vh_cube)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split monthly\n",
    "If you want to analyse your data under certain temporal aspects, in this case in a monthly manner, you can split up the original data cube into smaller monthly data cubes (if the data covers more than a month):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = sig0_cube.split_monthly()\n",
    "print(f\"Parent datacube has been split into {len(months)} monthly datacubes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that *yeoda* also provides convenience functions for yearly splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining\n",
    "\n",
    "If you have multiple datacubes, or have split them up to perform some processing, you can concatenate them using join operations. The following section will look closer at a few of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection\n",
    "\n",
    "You can use this operation to get only those fields of multiple datacubes with matching dimensions or with a specific matching dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_jan_remains = sig0_cube.intersect(months[0], on_dimension='time')\n",
    "only_jan_remains.inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union\n",
    "If you have two data cubes and you want to unite their information, you can simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_and_feb = months[0].unite(months[1])\n",
    "jan_and_feb.inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment\n",
    "\n",
    "The `align_dimension` method aligns a datacube with respect to a second datacube along a dimension (`name`). In other words, the order and the length of the dimension will then be the same. This also means that datacube entries are duplicated if they appear more often in the second datacube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a small test datacube\n",
    "small = sig0_cube.filter_by_dimension(datetime(2015, 2, 1, 4, 47, 30), name='time')\n",
    "print(f\"Small datacube of length {len(small)}\")\n",
    "# align the 'band' dimension with the large datacube\n",
    "aligned_with_duplicates = small.align_dimension(sig0_cube, 'pol')\n",
    "aligned_with_duplicates.inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "This section demonstrates how to load data. All functions have a common set of keyword arguments, where the most important ones are discussed here:\n",
    "\n",
    " - `band`: This argument specifies the band name as a string.\n",
    " - `dtype`: There are many types of *Python* data structures to store array-like data, and their selection mainly depends on what you want to do with the loaded data later on. These are offered by *yeoda*:\n",
    "   - xarray.DataSet (“xarray”)\n",
    "   - numpy.ndarray (“numpy”)\n",
    "   - pandas.DataFrame (“dataframe”)\n",
    " - `origin`: Depending on the chosen return data type, this parameter defines the origin of the pixel coordinates in the world system. The origin can be one of the following:\n",
    "   - upper right (“ur”, default)\n",
    "   - upper left (“ul”)\n",
    "   - lower right (“lr”)\n",
    "   - lower left (“ll”)\n",
    "   - center (\"c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load by geometry\n",
    "\n",
    "You can load data for a region defined by an arbitrary geometry, similar to how you can filter by a geometry. Geometries do not need to be axis-parallel, but data for the spanning axis aligned bounding box will be loaded nonetheless, to fit into an array data-structure. However, it is possible to mask any data outside the specified geometry by setting the `apply_mask` parameter to true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "polygon = [(10.6419,46.7977), (10.4689,47.2261), (11.3516,47.3510),\n",
    "           (11.3689,46.9161), (10.9426,46.9979)]\n",
    "\n",
    "months = sig0_cube.split_monthly()\n",
    "jan_vv = months[0].filter_spatially_by_geom(polygon, sref=sref)\\\n",
    "                  .filter_by_dimension(['VV'], name=\"pol\")\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    masked_xarray = jan_vv.load_by_geom(polygon, sref=sref, apply_mask=True, dtype=\"numpy\")\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.imshow(10 * np.log10(np.nanmean(10**(masked_xarray/10), axis=0)), cmap=plt.cm.Greys_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load by coordinates\n",
    "\n",
    "The `load_by_coords`, accepts a list of X and a list of Y (world system) coordinates as input. If the spatial reference of the coordinates is not equal to the data, you need to specify the spatial reference keyword argument `sref`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ogr\n",
    "\n",
    "# defining a point to sample a time series from\n",
    "point = ogr.Geometry(ogr.wkbPoint)\n",
    "point.AddPoint(16.210,47.242)\n",
    "point.AssignSpatialReference(sref)\n",
    "\n",
    "\n",
    "# load data by coordinates\n",
    "point_data = sig0_cube.filter_by_dimension(['VV'], name='pol')\n",
    "point_data.filter_spatially_by_geom(point, sref=sref, inplace=True)\n",
    "time_series = point_data.load_by_coords(point.GetX(), point.GetY(), band=1, sref=sref, dtype='numpy')\n",
    "\n",
    "# prepare data for graph\n",
    "x_vals = point_data[\"time\"].values\n",
    "y_vals = time_series.flatten()\n",
    "mask = np.isfinite(y_vals) # only use valid values\n",
    "\n",
    "# create a nice plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Backscatter timeseries')\n",
    "plt.plot(x_vals[mask], y_vals[mask], alpha=0.5)\n",
    "plt.scatter(x_vals[mask], y_vals[mask], s=15, color=\"red\", label=\"measurements\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('dB')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By specifying the `dtype` to be `\"numpy\"` parameter of the loading function, we request a plain *NumPy* array instead of the default *xarray*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load by pixels\n",
    "\n",
    "`load_by_pixels` expects pixel coordinates given by a list of row and column indexes. The keyword arguments `row_size` and `col_size` allow you to define a window, where the specified ranges count from left to right (columns) and from top to bottom (rows) starting at the given row and column coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter data cube for one day\n",
    "single_day = sig0_cube.filter_spatially_by_geom(point, sref=sref)\\\n",
    "                      .filter_by_dimension(datetime(2015, 1, 4, 5, 17, 16), name='time')\n",
    "\n",
    "pixels = single_day.load_by_pixels(0, 0, row_size=1200, col_size=1200, dtype=\"numpy\")\n",
    "\n",
    "# plot the data\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.title('Backscatter observed on 4.1.2015')\n",
    "img_h = plt.imshow(pixels[0, ...], cmap=plt.get_cmap(\"Greys_r\"))\n",
    "cb = plt.colorbar(img_h, shrink=0.6)\n",
    "cb.set_label(\"Sigma nought backscatter [dB]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "cbd1b77ab2c95540077e3b0dcf19906e8c91a69021f6ffa6867051d5cc14feb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
